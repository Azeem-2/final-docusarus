# P3-C4 Imitation Learning - Diagrams

This directory contains Mermaid diagrams for Chapter P3-C4: Imitation Learning.

## Diagram List

1. **figure-1-behavioral-cloning.mmd** - Behavioral cloning workflow showing expert demonstrations → supervised learning → policy deployment, with distribution shift failure mode.

2. **figure-2-bc-vs-irl-vs-dagger.mmd** - Comparison of three imitation learning approaches: behavioral cloning (direct policy), inverse RL (inferred reward → RL policy), and DAgger (iterative policy improvement).

3. **figure-3-multimodal-demos.mmd** - Multi-modal demonstration pipeline showing how vision, proprioception, and language inputs combine to train a robust policy.

4. **figure-4-imitation-rl-integration.mmd** - Integration of imitation learning with RL, showing how demonstrations initialize policies that are then improved through RL training.

## Style Notes

- Diagrams use consistent color palette from global style guide
- Conceptual focus (no detailed algorithms or equations)
- Emphasis on workflows and relationships between approaches
- Black & white readable (colors are supplementary)

## Usage

These diagrams support the conceptual introduction to imitation learning in Part 3, showing how it complements RL and fits into robotics simulation workflows.

