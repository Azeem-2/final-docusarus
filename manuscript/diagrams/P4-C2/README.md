# P4-C2 Multi-modal Models - Diagrams

This directory contains Mermaid diagrams for Chapter P4-C2: Multi-modal Models.

## Diagram List

1. **figure-1-multimodal-architecture.mmd** - Multi-modal model architecture showing vision encoder, language encoder, cross-modal fusion, and output tasks.

2. **figure-2-vlm-comparison.mmd** - Comparison of key vision-language models (LLaVA, GPT-Vision, Gemini, Qwen-VL, CLIP) with selection criteria.

3. **figure-3-language-to-action.mmd** - Language-to-action workflow from natural language command through VLM understanding, object grounding, planning, to control execution.

4. **figure-4-robotics-applications.mmd** - Overview of robotics applications: VQA, object grounding, language-to-action, and scene understanding.

## Style Notes

- Diagrams use consistent color palette from global style guide
- Conceptual focus (no detailed neural network architectures)
- Emphasis on workflows, comparisons, and applications
- Black & white readable (colors are supplementary)

## Usage

These diagrams support the practical introduction to multi-modal models in Part 4, showing how vision-language models enable natural human-robot interaction.

