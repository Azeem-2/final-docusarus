# Chapter Structure: P3-C3 Reinforcement Learning Basics

---
chapter_id: P3-C3
title: Reinforcement Learning Basics
version: v001
created: 2025-12-01
---

## Concept Density & Lesson Count

- **New concepts**: agent, environment, state, action, reward, episode, value function, policy, simple policy gradients, exploration.  
- **Prerequisites**: General probability/intuitive statistics; prior experience with control and dynamics (P2-C6, P2-C7) helpful but not required.  
- **Math depth**: Mostly conceptual, with light notation to define states, actions, and rewards; advanced derivations deferred to P4-C4.

Target: **3 lessons** emphasizing intuition and robotics-relevant examples over formal proofs.

---

## Pedagogical Progression (4 Layers)

1. **Layer 1 – Intuition**: Why RL is useful for robotics (learning behavior from experience).  
2. **Layer 2 – Concepts**: RL building blocks, rewards, value and policy ideas.  
3. **Layer 3 – Application**: Simple RL tasks in simulation (balancing, navigation, reaching).  
4. **Layer 4 – Integration**: Connection to advanced RL, safety, and sim-to-real.

---

## Lesson Breakdown

1. **Lesson 1 – RL Building Blocks and Rewards**  
2. **Lesson 2 – Value and Policy Intuition with Simple Examples**  
3. **Lesson 3 – RL for Robotics Simulation (Tasks, Exploration, and Safety Preview)**  

Each lesson follows the standard 6-part pattern (Hook, Theory, Walkthrough, Challenge, Takeaways, Learn with AI).


